{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 1 â€” Complete analysis (Populated)\n",
        "\n",
        "This notebook extends the base analysis with:\n",
        "- Rolling-window features (7-day and 30-day moving averages and vol)\n",
        "- Correlation heatmap between daily metrics and the Fear & Greed index\n",
        "- A short, transparent backtest using buy/sell imbalance signals (next-day PnL as outcome)\n",
        "\n",
        "Run all cells top-to-bottom. If running in Colab, upload the CSVs or mount Drive and set `DATA_DIR`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from IPython.display import display\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12,5)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "DATA_DIR = 'C:\\Users\\hulkh\\Downloads\\ds_Siva_Venkata_Bhanu_Prakash\\csv_files'\n",
        "print('DATA_DIR =', DATA_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load (or compute) daily aggregates\n",
        "This cell prefers an existing `/mnt/data/daily_aggregates.csv`. If it's not present it will try to compute daily aggregates from `historical_data.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "daily_path = os.path.join(DATA_DIR, 'daily_aggregates.csv')\n",
        "hist_path = os.path.join(DATA_DIR, 'historical_data.csv')\n",
        "fg_path = os.path.join(DATA_DIR, 'fear_greed_index.csv')\n",
        "\n",
        "if os.path.exists(daily_path):\n",
        "    daily = pd.read_csv(daily_path, parse_dates=['date'])\n",
        "    print('Loaded existing daily_aggregates.csv')\n",
        "else:\n",
        "    assert os.path.exists(hist_path), 'historical_data.csv not found to compute daily aggregates.'\n",
        "    hist = pd.read_csv(hist_path)\n",
        "    # Attempt to detect columns\n",
        "    time_col = next((c for c in ['time','Time','date','Date','timestamp','datetime','created_at'] if c in hist.columns), None)\n",
        "    if time_col is None:\n",
        "        time_col = next((c for c in hist.columns if 'time' in c.lower() or 'date' in c.lower()), None)\n",
        "    pnl_col = next((c for c in hist.columns if 'pnl' in c.lower()), None)\n",
        "    size_col = next((c for c in hist.columns if c.lower() in ['size','qty','quantity','volume']), None)\n",
        "    if size_col is None:\n",
        "        size_col = next((c for c in hist.columns if 'size' in c.lower() or 'qty' in c.lower() or 'volume' in c.lower()), None)\n",
        "    side_col = next((c for c in hist.columns if c.lower() in ['side','Side','buy_sell','direction','trade_type']), None)\n",
        "    if side_col is None:\n",
        "        side_col = next((c for c in hist.columns if 'side' in c.lower() or 'buy' in c.lower() or 'sell' in c.lower()), None)\n",
        "    # parse\n",
        "    hist['time_parsed'] = pd.to_datetime(hist[time_col], errors='coerce')\n",
        "    hist['date'] = hist['time_parsed'].dt.date\n",
        "    if pnl_col is None:\n",
        "        pnl_col = next((c for c in hist.columns if 'pnl' in c.lower()), None)\n",
        "    hist['closedPnL_clean'] = pd.to_numeric(hist[pnl_col], errors='coerce').fillna(0)\n",
        "    hist['size_clean'] = pd.to_numeric(hist[size_col], errors='coerce').fillna(0).abs()\n",
        "    if side_col is None:\n",
        "        hist['side_clean'] = hist['size_clean'].apply(lambda x: 'buy' if x>=0 else 'sell')\n",
        "    else:\n",
        "        hist['side_clean'] = hist[side_col].astype(str).str.lower().str.strip()\n",
        "    daily = hist.groupby('date').agg(\n",
        "        daily_closedPnL = ('closedPnL_clean', 'sum'),\n",
        "        daily_volume = ('size_clean', 'sum'),\n",
        "        buy_trades = ('side_clean', lambda s: (s=='buy').sum()),\n",
        "        sell_trades = ('side_clean', lambda s: (s=='sell').sum()),\n",
        "        total_trades = ('side_clean', 'count')\n",
        "    ).reset_index()\n",
        "    daily['date'] = pd.to_datetime(daily['date'])\n",
        "    daily = daily.sort_values('date')\n",
        "    daily.to_csv(daily_path, index=False)\n",
        "    print('Computed and saved daily_aggregates.csv')\n",
        "\n",
        "display(daily.head())\n",
        "print('\\nDaily rows:', len(daily))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rolling-window features (7-day and 30-day)\n",
        "Compute moving averages and volatilities for `daily_closedPnL` and `daily_volume`, and create a buy imbalance metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "daily = daily.sort_values('date').reset_index(drop=True)\n",
        "daily['pnl_ma7'] = daily['daily_closedPnL'].rolling(window=7, min_periods=1).mean()\n",
        "daily['pnl_ma30'] = daily['daily_closedPnL'].rolling(window=30, min_periods=1).mean()\n",
        "daily['pnl_vol7'] = daily['daily_closedPnL'].rolling(window=7, min_periods=1).std()\n",
        "daily['vol_ma7'] = daily['daily_volume'].rolling(window=7, min_periods=1).mean()\n",
        "daily['vol_ma30'] = daily['daily_volume'].rolling(window=30, min_periods=1).mean()\n",
        "daily['buy_imbalance'] = (daily['buy_trades'] - daily['sell_trades']) / daily['total_trades'].replace(0, np.nan)\n",
        "daily['buy_imbalance'] = daily['buy_imbalance'].fillna(0)\n",
        "\n",
        "display(daily[['date','daily_closedPnL','pnl_ma7','pnl_ma30','daily_volume','vol_ma7','buy_imbalance']].head(15))\n",
        "\n",
        "# Save rolling plots\n",
        "plt.figure()\n",
        "plt.plot(daily['date'], daily['daily_closedPnL'])\n",
        "plt.plot(daily['date'], daily['pnl_ma7'])\n",
        "plt.plot(daily['date'], daily['pnl_ma30'])\n",
        "plt.title('Daily Closed PnL with 7d & 30d MA')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closed PnL')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "pnl_ma_png = os.path.join(DATA_DIR, 'rolling_closed_pnl_ma.png')\n",
        "plt.savefig(pnl_ma_png)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(daily['date'], daily['daily_volume'])\n",
        "plt.plot(daily['date'], daily['vol_ma7'])\n",
        "plt.plot(daily['date'], daily['vol_ma30'])\n",
        "plt.title('Daily Volume with 7d & 30d MA')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Volume')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "vol_ma_png = os.path.join(DATA_DIR, 'rolling_volume_ma.png')\n",
        "plt.savefig(vol_ma_png)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation heatmap\n",
        "Build a correlation matrix between `daily_closedPnL`, `daily_volume`, `buy_imbalance`, and (if present) the Fear & Greed index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols = ['daily_closedPnL','daily_volume','buy_imbalance']\n",
        "if os.path.exists(fg_path):\n",
        "    fg = pd.read_csv(fg_path)\n",
        "    fg_date = next((c for c in fg.columns if 'date' in c.lower()), None)\n",
        "    fg_val = next((c for c in fg.columns if 'value' in c.lower() or 'index' in c.lower() or 'score' in c.lower()), None)\n",
        "    if fg_date is not None and fg_val is not None:\n",
        "        fg['date'] = pd.to_datetime(fg[fg_date], errors='coerce').dt.date\n",
        "        fg_daily = fg[[ 'date', fg_val ]].copy()\n",
        "        fg_daily.columns = ['date','fear_greed']\n",
        "        fg_daily['date'] = pd.to_datetime(fg_daily['date'])\n",
        "        merged = pd.merge(daily, fg_daily, on='date', how='left')\n",
        "        cols.append('fear_greed')\n",
        "    else:\n",
        "        merged = daily.copy()\n",
        "else:\n",
        "    merged = daily.copy()\n",
        "\n",
        "corr_df = merged[cols].corr()\n",
        "print('Correlation matrix:')\n",
        "display(corr_df)\n",
        "\n",
        "# Plot heatmap using matplotlib only\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.imshow(corr_df.values, aspect='auto')\n",
        "plt.colorbar()\n",
        "plt.xticks(ticks=range(len(cols)), labels=cols, rotation=45, ha='right')\n",
        "plt.yticks(ticks=range(len(cols)), labels=cols)\n",
        "plt.title('Correlation heatmap')\n",
        "heatmap_png = os.path.join(DATA_DIR, 'correlation_heatmap.png')\n",
        "plt.tight_layout()\n",
        "plt.savefig(heatmap_png)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Brief backtest using buy/sell imbalance\n",
        "Signal definition (simple & transparent):\n",
        "- `buy_imbalance = (buy_trades - sell_trades) / total_trades`.\n",
        "- Long signal (1) when `buy_imbalance > 0.10` and `daily_volume > vol_ma7`.\n",
        "- Short signal (-1) when `buy_imbalance < -0.10` and `daily_volume > vol_ma7`.\n",
        "\n",
        "We use **next-day closed PnL** as the outcome for the signal (this is a coarse proxy; in a real backtest you would use trade-level fills/prices)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = merged.sort_values('date').reset_index(drop=True).copy()\n",
        "df['signal'] = 0\n",
        "df.loc[(df['buy_imbalance'] > 0.10) & (df['daily_volume'] > df['vol_ma7']), 'signal'] = 1\n",
        "df.loc[(df['buy_imbalance'] < -0.10) & (df['daily_volume'] > df['vol_ma7']), 'signal'] = -1\n",
        "\n",
        "df['next_day_pnl'] = df['daily_closedPnL'].shift(-1)\n",
        "df['strategy_pnl'] = df['signal'] * df['next_day_pnl']\n",
        "df['strategy_cum'] = df['strategy_pnl'].cumsum().fillna(method='ffill').fillna(0)\n",
        "\n",
        "total_pnl = df['strategy_pnl'].sum(skipna=True)\n",
        "num_trades = (df['signal']!=0).sum()\n",
        "avg_pnl_per_trade = df.loc[df['signal']!=0, 'strategy_pnl'].mean()\n",
        "win_rate = (df.loc[df['signal']!=0, 'strategy_pnl'] > 0).mean()\n",
        "sharpe = None\n",
        "if df['strategy_pnl'].std() != 0:\n",
        "    sharpe = (df['strategy_pnl'].mean() / df['strategy_pnl'].std()) * (252**0.5)\n",
        "\n",
        "print('Backtest summary (simple):')\n",
        "print('Total PnL (sum of signal * next_day_pnl):', total_pnl)\n",
        "print('Number of signals:', num_trades)\n",
        "print('Avg PnL per signal:', avg_pnl_per_trade)\n",
        "print('Win rate:', win_rate)\n",
        "print('Sharpe (ann. approx):', sharpe)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(df['date'], df['strategy_cum'])\n",
        "plt.title('Strategy cumulative PnL (signal * next-day PnL)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative PnL')\n",
        "plt.grid(True)\n",
        "strategy_png = os.path.join(DATA_DIR, 'strategy_cumulative_pnl.png')\n",
        "plt.tight_layout()\n",
        "plt.savefig(strategy_png)\n",
        "plt.show()\n",
        "\n",
        "signals_path = os.path.join(DATA_DIR, 'daily_signals_backtest.csv')\n",
        "df.to_csv(signals_path, index=False)\n",
        "print('\\nSaved signals and results to', signals_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save all artifacts and final notes\n",
        "The notebook saved plots and CSVs to `/mnt/data/` so you can download them. Remember this backtest is intentionally simple and uses next-day realized PnL as the outcome â€” for production you would use precise fills/prices and transaction costs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "artifacts = [pnl_ma_png, vol_ma_png, heatmap_png, strategy_png, signals_path, daily_path]\n",
        "for a in artifacts:\n",
        "    print('-', a, 'exists?', os.path.exists(a))\n",
        "\n",
        "print('\\nNotebook generation completed at', datetime.now().isoformat())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----\n",
        "### End of populated notebook\n",
        "\n",
        "If you'd like additional expansions (transaction-cost-aware backtest, more sophisticated signals, intraday analysis, or automated report generation), tell me which one and I'll extend the notebook further."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "prj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
